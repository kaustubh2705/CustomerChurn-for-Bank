{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information\n",
    "Predict if a customer leave the bank:  \n",
    "Churn rate of a bank: the annual percentage rate at which customers stop subscribing to a service.  \n",
    "\n",
    "The columns in order:   \n",
    "Row Number  \n",
    "Customer ID  \n",
    "Lastname  \n",
    "Credit Score  \n",
    "Geography  \n",
    "Gender  \n",
    "Age  \n",
    "Tenure  \n",
    "Balance  \n",
    "Number of Products  \n",
    "Has Credit Card  \n",
    "Is Active Member  \n",
    "Estimated Salary  \n",
    "Exited: They wait 6 months and enter the exit information for each customer.  \n",
    "\n",
    "You can apply this model to any dataset with lots of variables and two outcome : 0 or 1\n",
    "\n",
    "This is a classification problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) IMPORT THE DATASET\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 3:13].values # we excluded the row number, customer id, surname columns. they have no impact for a customer to decide to leave or stay. \n",
    "y = dataset.iloc[:, 13].values #the targets, actual outputs\n",
    "#.values: runs only the values, no headings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, this is a classification problem, we can use the classification template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification template\n",
    "\n",
    "#According to template, after importing the dataset, and creating our dependent and independent variables\n",
    "#we need to split the dataset into the training and test sets, however for this dataset\n",
    "#we, first, need to encode the categorical variables:\n",
    "\n",
    "# 2) ENCODING CATEGORICAL DATA: CATEGORICAL TEMPLATE\n",
    "#For this we have categorical template. This section will endcode any categorical data that you have in your dataset:\n",
    "\n",
    "# Encoding the Independent Variable: we have two categorical variables\n",
    "#country and gender\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "#Country column number is 1. This will encode the values as 0, 1, 2...\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "#Gender column number is 2.\n",
    "#we are creating dummy variables for the country column\n",
    "#the reason for that we don't want any numbers except 0 or 1. \n",
    "#so we will say for example France = 0,0,1; Spain= 1,0,0; German; 0,1,0 instead of  0,1,2\n",
    "#one hot encoder will create three dummy columns, place it before the credit score column.\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "#we don't need three variables to represent the three different countries, so\n",
    "#we will delete one dummy variable to avoid falling into the dummy variable trap.\n",
    "#this won't change the country information: France becomes = 0,0; German=0,1; Spain,1,0\n",
    "X = X[:, 1:] #this takes all the rows in X (inputs), and takes columns from 1 to end\n",
    "#CHANGED: just the column numbers, and one delete one dummy variable\n",
    "\n",
    "# 3) SPLITTING THE DATASET INTO TRAINING AND TEST:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "#CHANGED: just the test size from 0.25 to 0.2 since we have 10,000 observations\n",
    "#to avoid warning: we changed from sklearn.cross_validation to model_selection\n",
    "\n",
    "# 4) FEATURE SCALING: \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "#CHANGED: No change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Start building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/nevinyilmaz/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.1)`\n",
      "/Users/nevinyilmaz/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:35: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 2s 264us/step - loss: 0.4973 - acc: 0.7951\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.4375 - acc: 0.7960\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 2s 267us/step - loss: 0.4339 - acc: 0.7960\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 2s 303us/step - loss: 0.4316 - acc: 0.7960\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 2s 274us/step - loss: 0.4292 - acc: 0.7960\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 2s 229us/step - loss: 0.4260 - acc: 0.7960\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 2s 238us/step - loss: 0.4287 - acc: 0.8056\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 2s 244us/step - loss: 0.4282 - acc: 0.8202\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 2s 258us/step - loss: 0.4284 - acc: 0.8205\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 2s 269us/step - loss: 0.4261 - acc: 0.8245\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 2s 282us/step - loss: 0.4283 - acc: 0.8265\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 2s 283us/step - loss: 0.4266 - acc: 0.8246\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 2s 256us/step - loss: 0.4246 - acc: 0.8275\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 2s 256us/step - loss: 0.4268 - acc: 0.8262\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 2s 239us/step - loss: 0.4273 - acc: 0.8269\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 2s 247us/step - loss: 0.4285 - acc: 0.8249\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 2s 264us/step - loss: 0.4265 - acc: 0.8294 0s - loss:\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 2s 245us/step - loss: 0.4257 - acc: 0.8286\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 0.4255 - acc: 0.8250\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 2s 216us/step - loss: 0.4291 - acc: 0.8276\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 2s 288us/step - loss: 0.4258 - acc: 0.8251\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 2s 241us/step - loss: 0.4249 - acc: 0.8284\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 2s 233us/step - loss: 0.4254 - acc: 0.8262\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 2s 250us/step - loss: 0.4260 - acc: 0.8274\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 2s 216us/step - loss: 0.4249 - acc: 0.8282\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 0.4268 - acc: 0.8291\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 0.4248 - acc: 0.8294\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 0.4254 - acc: 0.8264\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 0.4254 - acc: 0.8295\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 0.4240 - acc: 0.8261\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 0.4220 - acc: 0.8275\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 0.4300 - acc: 0.8299\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 0.4258 - acc: 0.8306\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 0.4254 - acc: 0.8297\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 2s 216us/step - loss: 0.4235 - acc: 0.8296\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.4218 - acc: 0.8295\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 2s 264us/step - loss: 0.4267 - acc: 0.8290\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 2s 300us/step - loss: 0.4214 - acc: 0.8289 0s - loss: 0.4218 - acc: 0.\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 2s 291us/step - loss: 0.4252 - acc: 0.8302\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 2s 230us/step - loss: 0.4205 - acc: 0.8284\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 2s 262us/step - loss: 0.4222 - acc: 0.8285\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 2s 281us/step - loss: 0.4254 - acc: 0.8304\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.4242 - acc: 0.8306\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 2s 246us/step - loss: 0.4234 - acc: 0.8297\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 2s 237us/step - loss: 0.4220 - acc: 0.8272\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.4267 - acc: 0.8287\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 2s 227us/step - loss: 0.4255 - acc: 0.8309\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 2s 290us/step - loss: 0.4253 - acc: 0.8319\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 2s 273us/step - loss: 0.4222 - acc: 0.8284\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 2s 237us/step - loss: 0.4218 - acc: 0.8296\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 2s 257us/step - loss: 0.4260 - acc: 0.8299\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 2s 295us/step - loss: 0.4235 - acc: 0.8297\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 2s 291us/step - loss: 0.4229 - acc: 0.8300\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 2s 268us/step - loss: 0.4260 - acc: 0.8289\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 2s 305us/step - loss: 0.4219 - acc: 0.8290\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 2s 285us/step - loss: 0.4239 - acc: 0.8279\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 2s 266us/step - loss: 0.4230 - acc: 0.8284\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 3s 325us/step - loss: 0.4241 - acc: 0.8314\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 2s 298us/step - loss: 0.4264 - acc: 0.8299\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 2s 269us/step - loss: 0.4219 - acc: 0.8314\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 2s 268us/step - loss: 0.4251 - acc: 0.8306\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 2s 306us/step - loss: 0.4239 - acc: 0.8325\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 3s 329us/step - loss: 0.4260 - acc: 0.8290\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 3s 322us/step - loss: 0.4261 - acc: 0.8289\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 2s 297us/step - loss: 0.4264 - acc: 0.8297\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 2s 262us/step - loss: 0.4200 - acc: 0.8297\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 2s 262us/step - loss: 0.4249 - acc: 0.8299\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 2s 261us/step - loss: 0.4275 - acc: 0.8271\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 2s 266us/step - loss: 0.4221 - acc: 0.8280\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 2s 277us/step - loss: 0.4246 - acc: 0.8295\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 3s 332us/step - loss: 0.4236 - acc: 0.8294\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 2s 275us/step - loss: 0.4254 - acc: 0.8304\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 2s 307us/step - loss: 0.4236 - acc: 0.8301\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 2s 300us/step - loss: 0.4251 - acc: 0.8269\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 2s 307us/step - loss: 0.4227 - acc: 0.8305\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 2s 296us/step - loss: 0.4256 - acc: 0.8291\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 2s 292us/step - loss: 0.4262 - acc: 0.8300\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 2s 293us/step - loss: 0.4239 - acc: 0.8291\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 2s 280us/step - loss: 0.4244 - acc: 0.8275\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 2s 280us/step - loss: 0.4223 - acc: 0.8280\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 2s 298us/step - loss: 0.4234 - acc: 0.8309\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 2s 300us/step - loss: 0.4269 - acc: 0.8277\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 2s 293us/step - loss: 0.4262 - acc: 0.8305\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 2s 282us/step - loss: 0.4241 - acc: 0.8284\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 2s 278us/step - loss: 0.4197 - acc: 0.8315\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 2s 277us/step - loss: 0.4224 - acc: 0.8287\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 2s 274us/step - loss: 0.4265 - acc: 0.8306\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.4241 - acc: 0.8312\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 2s 242us/step - loss: 0.4225 - acc: 0.8277\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 2s 299us/step - loss: 0.4260 - acc: 0.8289\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 2s 287us/step - loss: 0.4268 - acc: 0.8302\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 2s 273us/step - loss: 0.4241 - acc: 0.8297\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 2s 281us/step - loss: 0.4214 - acc: 0.8304\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 2s 292us/step - loss: 0.4213 - acc: 0.8329\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 2s 296us/step - loss: 0.4227 - acc: 0.8306\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 2s 281us/step - loss: 0.4246 - acc: 0.8299\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 2s 283us/step - loss: 0.4205 - acc: 0.8310\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 2s 279us/step - loss: 0.4225 - acc: 0.8319\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 2s 278us/step - loss: 0.4246 - acc: 0.8327\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 2s 279us/step - loss: 0.4214 - acc: 0.8316\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10dd5c710>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we're going to import the Keras libraries and packages for the model\n",
    "#This part is not included in the classification template\n",
    "\n",
    "#1) IMPORT LIBRARIES\n",
    "import keras\n",
    "from keras.models import Sequential #required to initialize the ANN\n",
    "from keras.layers import Dense #required to build layers\n",
    "from keras.layers import Dropout #improving the ANN\n",
    "# 2) INITIALIZE THE ANN\n",
    "#we need to initialize the ANN, that is defining it as a sequence of layers\n",
    "classifier = Sequential()\n",
    "#this object we created is nothing else than the model itself\n",
    "#since this model is going to be a classifier model, we will just name it classifier\n",
    "#so this classifier object is nothing else but the future model that we're going to create\n",
    "\n",
    "# 3) ADD THE LAYERS:\n",
    "#adding input and the first hidden layer with dropouts (with dropout addition):\n",
    "classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=11))\n",
    "classifier.add(Dropout(p=0.1))\n",
    "#dropout argument:\n",
    "#p=fraction you want to disable. \n",
    "#if we have 10 neurons and if we put p=0.1, that means we will disable 1 neuron in each iteration\n",
    "#try higher value of p if you still have overfitting\n",
    "#dense function arguments:\n",
    "#output_dim(units): the number of nodes/neurons you want to add in this hidden layer.\n",
    "#it's the average of the number of nodes in the input layer and the output layer\n",
    "#in this dataset, we have 11 columns=nodes in input layer; we have 1 node in the output layer=1 column\n",
    "#the average is 11+1=12/2=6 nodes in the hidden layer\n",
    "#init: randomly initialize the weights\n",
    "#activstion = activation function\n",
    "#input_dim = number of independent variables = columns\n",
    "\n",
    "#adding the second hidden layer:\n",
    "classifier.add(Dense(units = 6, kernel_initializer='uniform', activation = 'relu'))\n",
    "classifier.add(Dropout(p=0.1))\n",
    "#adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer='uniform', activation = 'sigmoid'))\n",
    "#if you're dealing with a dependent variable (output) that has more than two categories (0,1),\n",
    "#like for example, 3 categories, you will need to change two things:\n",
    "#output_dim = 3 (because the output will be one-hot encoded)\n",
    "#activation = softmax\n",
    "\n",
    "#4) COMPILE THE ANN\n",
    "classifier.compile(optimizer = 'adam', loss= 'binary_crossentropy', metrics = ['accuracy'])\n",
    "#compile arguments:\n",
    "#optimizer: weights are still only initialize. this algorithm changes the weights efficiently\n",
    "#loss: since we have 2 outcomes that are binary we use binary_crossentropy\n",
    "#if you have more than 2 outcomes than loss = categorical_crossentropy\n",
    "#metrics:creatorion that you choose to evaluate your model.\n",
    "\n",
    "#5) FITTING THE ANN TO THE TRAINING SET\n",
    "classifier.fit(X_train, y_train, batch_size=10, epochs= 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3:Making the predictions and evaluating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification template continue\n",
    "\n",
    "\n",
    "# 1) PREDICTING THE TEST SET RESULTS\n",
    "y_pred = classifier.predict(X_test)\n",
    "#y_pred is the all the probabilities that the 2000 customers of the test set to leave the bank\n",
    "#y_pred first row: 0.23, so it says the first customer has 20% prob of leaving\n",
    "#bank can use this information: sort the y_pred descending, find out the 10% of the customers that are most likely to leave\n",
    "#then try to understand the why these customers are most likely to leave\n",
    "\n",
    "# 2) MAKING THE CONFUSION MATRIX\n",
    "#in order to use the confusion matrix we need y_pred as true or false not the probabilities:\n",
    "#convert the probabilities into the predicted results:\n",
    "#we need to choose a threshold to decide when the predicted result is 1 and 0\n",
    "#natural threshold to take is 0.5. if we are dealing with more sensitive information we need to take higher threshold.\n",
    "#for example, if a tumor is malignant\n",
    "y_pred = (y_pred > 0.5) #if y_pred is >0.5 then it returns true, otherwise false\n",
    "# lower than 0.5 don't leave the bank, higher than 0.5 leave the bank\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1562,   33],\n",
       "       [ 288,  117]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's look at the confusion matrix\n",
    "cm\n",
    "#so out of 2000 new observations we get 1524 + 161 correct predictions, and 71 +244 false predictions\n",
    "#this will change in each run of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.950000000000003"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy = ((cm[0,0] + cm[1,1]) / 2000) *100\n",
    "test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's try to predict if this customer will leave the bank or not, using our model:\n",
    "#Geography: France\n",
    "#Credit Score: 600\n",
    "#Gender: Male\n",
    "#Age: 40 years old\n",
    "#Tenure: 3 years\n",
    "#Balance: $60000\n",
    "#Number of Products: 2\n",
    "#Does this customer have a credit card ? Yes\n",
    "#Is this customer an Active Member: Yes\n",
    "#Estimated Salary: $50000\n",
    "new_prediction = classifier.predict(sc.transform(np.array([[0.0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])))\n",
    "new_prediction = (new_prediction>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False]], dtype=bool)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_prediction\n",
    "#false=meaning the customer will not leave the bank"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "527px",
    "left": "110px",
    "right": "20px",
    "top": "82px",
    "width": "782px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
